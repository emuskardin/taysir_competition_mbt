{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TAYSIR Baseline for Track 2- Extraction of WA from Recurrent Neural Net already Trained on a Language Modelling Task\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to let you play around with the Weighted Automata extraction baseline that use the spectral extraction technique.\n",
    "Inputed Neural Net can be LSTM, GRU or SRN network, after which it will draw a neat little WA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "## Imports and version verifying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q mlflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "\n",
    "print(\"Your torch version:\", torch.__version__)\n",
    "print(\"Your mlflow version:\", mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "\n",
    "## Choosing the task\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 2\n",
    "DATASET = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the RNN of the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"models/{TRACK}.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of some variables that would be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_letters = model.input_size -1\n",
    "cell_type = model.cell_type\n",
    "\n",
    "print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "print(\"The type of the recurrent cells is\", cell_type.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and have an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"datasets/{TRACK}.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "sequences = []\n",
    "with open(file) as f:\n",
    "    f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the trained RNN. It is given as a MLFlow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HewNg6Ew6Jz"
   },
   "source": [
    "# Model extraction\n",
    "## Seeding\n",
    "We are seeding for reproductibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the trained RNN. It is given as a MLFlow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WA Extraction Baseline\n",
    "This part is the one you need to change to put your own algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm is the spectral extraction described in this paper: https://arxiv.org/abs/2009.13101\n",
    "\n",
    "We are going to fix a number of prefixes (and of suffixes) and then use the model to generate these numbers of elements. The model will then be used to fill the Hankel matrix from which we will create a Weighted Automaton. \n",
    "\n",
    "This first function could be useful for other approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_word(model, max_len, nb_letters):\n",
    "    \"\"\" A function that uses the LM-RNN to generate a sequence\"\"\"\n",
    "\n",
    "    current_symbol = nb_letters - 2 #start symbol is always that integer\n",
    "    gen_word = [current_symbol]\n",
    "    len_word = 1\n",
    "    current_hidden = None #Initial state is defined that way\n",
    "    with torch.no_grad():\n",
    "        while len_word < max_len and current_symbol != nb_letters - 1: #end symbol is always nb_letters - 1\n",
    "            current_one_encoded = model.one_hot_encode([current_symbol])\n",
    "            \"\"\"Despite its name, in LM task, this is the function that provides the probability of the next symbol given a prefix\"\"\"\n",
    "            out, current_hidden = model.forward_bin(current_one_encoded, current_hidden)\n",
    "\n",
    "            \"\"\"Sample next letter acording to RNN next symbol distribution\"\"\"\n",
    "            current_symbol = torch.multinomial(out, 1).item()\n",
    "            gen_word += [current_symbol]\n",
    "            len_word +=1\n",
    "    \n",
    "    \"\"\"Make sure the last symbol is the end of sequence one\"\"\"\n",
    "    if len_word == max_len and current_symbol!=nb_letters - 1:\n",
    "        gen_word +=[nb_letters - 1]\n",
    "    \n",
    "    return gen_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our algorithm relies on the use of the toolbox scikit-splearn (https://remieyraud.github.io/scikit-splearn/) that can be installed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-splearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are needed for our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_prefixes(prefixes_set, word):\n",
    "    \"\"\" add all prefixes of a word to an existing set of prefixes\"\"\"\n",
    "    for i in range(2, len(word)):\n",
    "        prefixes_set.add(tuple(word[:i]))\n",
    "def add_all_suffixes(suffixes_set, word):\n",
    "    \"\"\" add all suffixes of a word to an existing set of prefixes\"\"\"\n",
    "    for i in range(2, len(word)):\n",
    "        suffixes_set.add(tuple(word[i:]))\n",
    "\n",
    "def generate_basis(model, nb_prefixes, nb_suffixes, max_len, nb_letters):\n",
    "    \"\"\"A function to generate a set of prefixes and suffixes to be fed to the model to build the Hankel matrix\"\"\"\n",
    "    words = set()\n",
    "    prefixes = set()\n",
    "    suffixes = set()\n",
    "    with torch.no_grad():\n",
    "        while (len(prefixes) < nb_prefixes or len(suffixes) < nb_suffixes):\n",
    "            gen_word = tuple(generate_one_word(model, max_len, nb_letters))\n",
    "            if gen_word not in words:\n",
    "                words.add(gen_word)\n",
    "                if len(prefixes) < nb_prefixes:\n",
    "                    add_all_prefixes(prefixes, list(gen_word))\n",
    "                if len(suffixes) < nb_suffixes:\n",
    "                    add_all_suffixes(suffixes, list(gen_word))\n",
    "                        \n",
    "    # it is better to sort for the Hankel construction and if the lists start by the delimiting symbols\n",
    "    rows = [(nb_letters-2,)] + sorted(list(prefixes), key=lambda t: (len(t), t[0]))\n",
    "    columns = [(nb_letters-1,)] + sorted(list(suffixes), key=lambda t: (len(t), t[0]))\n",
    "    \n",
    "    # need to create the set of all the words this basis implies to ask the RNN\n",
    "    letters = [[]] + [[i] for i in range(nb_letters)]\n",
    "    all_combinations= set()\n",
    "    for letter in letters:\n",
    "            for prefix in rows:\n",
    "                for suffix in columns:\n",
    "                    all_combinations.add(tuple(list(prefix) + letter + list(suffix)))\n",
    "    return rows, columns, list(all_combinations)\n",
    "\n",
    "def get_values(model, all_combinations, nb_letters):\n",
    "    \"\"\" returns a dictionary with all words in all_combinations as keys and corresponding RNN assigned values\"\"\"\n",
    "    probas = dict()\n",
    "    for word in all_combinations:\n",
    "        one_hot_word = model.one_hot_encode(list(word))\n",
    "        value = model.predict(one_hot_word)\n",
    "        probas[tuple(word)] = value\n",
    "\n",
    "    return probas\n",
    "\n",
    "def create_hankels(model, prefixes, suffixes, all_combinations, nb_letters):\n",
    "    \"\"\"\n",
    "    Redefinition of hankels(): return the list of matrices needed for extracting WA\n",
    "    :param model: a RNN in pytorch\n",
    "    :param prefixes: the list of prefixes (for rows)\n",
    "    :param suffixes: the list of suffixes (for columns)\n",
    "    :param all_combinaisons: a list of all the words whose value have to be asked to the RNN\n",
    "    :param nb_letters: the number of letters of the problem\n",
    "    \n",
    "    :return: a list of matrices lhankels. lhankels[0] is the Hankel matrice while\n",
    "             lhankel[i] is H_{i-1}: lhankel[i][prefix][suffix]=predict(prefix + [i] + suffix)\n",
    "    \"\"\"\n",
    "    print(\"Computing Hankels...\")\n",
    "    words_probas = get_values(model, all_combinations, nb_letters)\n",
    "    print(\"    Done using the model\")\n",
    "    \n",
    "    lhankels = [np.zeros((len(prefixes), len(suffixes))) for _ in range(nb_letters+1)]\n",
    "    # empty string and letters matrices:\n",
    "    letters = [[]] + [[i] for i in range(nb_letters)]\n",
    "    for letter in range(len(letters)):\n",
    "        for l in range(len(prefixes)):\n",
    "            for c in range(len(suffixes)):\n",
    "                p = words_probas[prefixes[l] + tuple(letters[letter]) + suffixes[c]]\n",
    "                lhankels[letter][l][c] = p\n",
    "    print(\"    Done computing Hankels\")\n",
    "    return lhankels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can define our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splearn as sp\n",
    "from numpy.linalg import svd, pinv\n",
    "def spectral_distillation(model, nb_states, nb_prefixes, nb_suffixes, max_len, nb_letters):\n",
    "    \"\"\"\n",
    "        Extract a WA of given rank from a RNN\n",
    "        \n",
    "        :param model: a pytorch recurrent model\n",
    "        :param nb_states: the rank of the WA to be extracted\n",
    "        :param nb_prefixes: number of prefixes for the basis \n",
    "        :param nb_suffixes: number of suffixes for the basis \n",
    "        :param max_len: the maximal size of the sequences generated \n",
    "        :param nb_letters: the number of different symbols \n",
    "        \n",
    "        :results: return the distiled weighted automata  \n",
    "    \"\"\"\n",
    "    prefixes, suffixes, all_combinaisons = generate_basis(model, nb_prefixes, nb_suffixes, max_len, nb_letters)\n",
    "    hankels = create_hankels(model, prefixes, suffixes, all_combinaisons, nb_letters)\n",
    "    \"\"\"Computing the SVD\"\"\"\n",
    "    hankel = hankels[0]\n",
    "    [u, s, v] = svd(hankel)\n",
    "    \n",
    "    u = u[:, :nb_states]\n",
    "    v = v[:nb_states, :]\n",
    "    ds = np.diag(s[:nb_states])\n",
    "    \n",
    "    #Computing WA elements\n",
    "    pis = pinv(v)\n",
    "    del v\n",
    "    pip = pinv(np.dot(u, ds))\n",
    "    del u, ds\n",
    "    init = np.dot(hankel[0, :], pis)\n",
    "    term = np.dot(pip, hankel[:, 0])\n",
    "    transitions = []\n",
    "    for x in range(nb_letters):\n",
    "        hankel = hankels[x+1]\n",
    "        transitions.append(np.dot(pip, np.dot(hankel, pis)))\n",
    "    \n",
    "    WA = sp.Automaton(nbL=nb_letters, nbS=nb_states, initial=init, final=term, transitions=transitions, type=\"classic\")\n",
    "    return WA\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WA = spectral_distillation(model, 2, 2, 2, 10, nb_letters)\n",
    "print(\"Number of states of the extracted WA:\", WA.initial.shape[0])\n",
    "print(\"Output on example:\", WA.val(sequences[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "The only thing to do is to define a function that takes a sequence as a list of integers and returns the value given to this sequence to the sequence. Your model is **NOT** a parameter of this function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq):\n",
    "    return WA.val(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive**  that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit_tools import save_function\n",
    "\n",
    "save_function(predict, alphabet_size=nb_letters, prefix=f'dataset{TRACK}.{DATASET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For fun, show WA graphical representation\n",
    "You may need to install the graphviz library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = WA.get_dot(threshold = 0.01, title = 'dotfile')\n",
    "# To display the dot string, one can use graphviz:\n",
    "from graphviz import Source\n",
    "src = Source(dot)\n",
    "src.render('dotfile' + '.gv', view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
