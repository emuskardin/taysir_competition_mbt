{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TAYSIR Baseline for Track 1- Extraction of DFA from Recurrent Neural Net already Trained on a Binary Classification Task\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This is a notebook to let you play around with the Deterministic Finite Automata (DFA) extraction baseline that use the k-means clustering algorithm on the hidden states of the model encountered while parsing the validation set.\n",
    "\n",
    "Inputed Neural Net can be LSTM, GRU or SRN network, after which it will draw a neat little DFA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "## Imports and version verifying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q mlflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOuRRA0OFIhG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "\n",
    "print(\"Your torch version:\", torch.__version__)\n",
    "print(\"Your mlflow version:\", mlflow.__version__)\n",
    "import sys\n",
    "print(\"Your python version:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested with:\n",
    "* Torch version: 1.11.0+cu102\n",
    "* MLFlow version: 1.25.1\n",
    "* Python version: 3.8.10 [GCC 9.4.0]\n",
    "\n",
    "Python versions starting at 3.7 are supposed to work (but have not been tested).\n",
    "\n",
    "## Choosing the task\n",
    "\n",
    "First you must select one of the phases/datasets we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = 1\n",
    "DATASET = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the RNN of the competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"models/{TRACK}.{DATASET}.taysir.model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of some variables that would be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_letters = model.input_size -1\n",
    "cell_type = model.cell_type\n",
    "\n",
    "print(\"The alphabet contains\", nb_letters, \"symbols.\")\n",
    "print(\"The type of the recurrent cells is\", cell_type.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The input data is in the following format :\n",
    "\n",
    "```\n",
    "[Number of sequences] [Alphabet size]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "[Length of sequence] [List of symbols]\n",
    "...\n",
    "[Length of sequence] [List of symbols]\n",
    "```\n",
    "\n",
    "For example the following data :\n",
    "\n",
    "```\n",
    "5 10\n",
    "6 8 6 5 1 6 7 4 9\n",
    "12 8 6 9 4 6 8 2 1 0 6 5 9\n",
    "7 8 9 4 3 0 4 9\n",
    "4 8 0 4 9\n",
    "8 8 1 5 2 6 0 5 3 9\n",
    "```\n",
    "\n",
    "is composed of 5 sequences and have an alphabet size of 10 (so symbols are between 0 and 9) and the first sequence is composed of 6 symbols (8 6 5 1 6 7 4 9), notice that 8 is the start symbol and 9 is the end symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"datasets/{TRACK}.{DATASET}.taysir.valid.words\"\n",
    "\n",
    "sequences = []\n",
    "with open(file) as f:\n",
    "    f.readline() #Skip first line (number of sequences, alphabet size)\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        seq = line.split(' ')\n",
    "        seq = [int(i) for i in seq[1:]] #Remove first value (length of sequence) and cast to int\n",
    "        sequences.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *sequences* is thus **a list of lists**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sequences:', len(sequences))\n",
    "print('10 first sequences:')\n",
    "for i in range(10):\n",
    "    print(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the trained RNN. It is given as a MLFlow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HewNg6Ew6Jz"
   },
   "source": [
    "# Model extraction\n",
    "## Seeding\n",
    "We are seeding for reproductibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the latent states\n",
    "We first define a function that computes and stores the latent states reached by the RNN during the processing of the available data. \n",
    "\n",
    "Notice that it is possible to batch this function and to use GPU, but we provide a basic version so that it would be more clear to understand how our RNN can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent(model, sequences):\n",
    "    \"\"\"\n",
    "    This function parses the dataset into the model and keeps track of the hidden state of the model it saw during the process\n",
    "    returns the observed hidden states\n",
    "    input: the MLFlow model, the dataset\n",
    "    output: observed hidden states in a matrix\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  \n",
    "        h_lists = []\n",
    "        \n",
    "        for w in sequences:\n",
    "            \n",
    "            one_hot_w = model.one_hot_encode(w)\n",
    "            \n",
    "            \"\"\"Get tensors of hidden states encountered at each layer\"\"\"\n",
    "            h_tensors = model.reached_hidden(one_hot_w, None)\n",
    "            \"\"\"\n",
    "            h_tensor is a list of list of states. The number of lists is the number of words (here one)\n",
    "                Each list is made of a number of tensors corresponding to the number of hidden layer\n",
    "                    Each tensor contains the hidden states encountered at this layer while parsing the word. \n",
    "                    Dimension:(1, w_length, size of the layer) \n",
    "            \"\"\" \n",
    "            h_tensors = h_tensors[0] # because we just passed one word\n",
    "            if model.task == 'lm':\n",
    "                h_tensors = h_tensors[-1] # to use the full string, not a prefix \n",
    "            \n",
    "            \"\"\"We need to go through the layers to get the states of the model, that is, the vectors concatenating the states of each layers\"\"\"\n",
    "            hiddens = []\n",
    "            for j in range(model.n_layers):\n",
    "                if model.hides_pairs: #True if LSTM, False otherwise \n",
    "                    (h,c) = h_tensors[j]\n",
    "                    #h contains the vectors outputed by the layer at each timestep/length, c contains the corresponding carries\n",
    "                    \n",
    "                    \"\"\"Get vectors concatening h and c for all prefix lengths\"\"\"\n",
    "                    #the squeeze got rid of the batch size dimension (here valued 1), the unbind allows to have one vector per prefix length\n",
    "                    l = list(zip(h.squeeze(0).unbind(0),\n",
    "                                c.squeeze(0).unbind(0)))\n",
    "                    #l contains the list of (h, c) encountered at this layer\n",
    "                    l= [torch.cat(x) for x in l]\n",
    "                    #Now l contains vectors corresponding to the concatenation of h and c at each prefix length\n",
    "\n",
    "                else:\n",
    "                    #non LSTM\n",
    "                    h = h_tensors[j]\n",
    "                    l = h.squeeze(0).unbind(0)\n",
    "                    \n",
    "                hiddens.append(l)\n",
    "                    \n",
    "            \"\"\"Merge the different lists: the state of a RNN is the concatenantion of the states of its recurrent layers\"\"\"\n",
    "            ls = list(map(list, zip(*hiddens))) # transpose\n",
    "            ls = [torch.cat(x) for x in ls]\n",
    "            #Make the vectors of hidden states into a matrix:\n",
    "            ls = torch.stack(ls)\n",
    "            \n",
    "            h_lists += [ls]\n",
    "            # So h_lists contains tensors, each one containing the hidden states encountered for a given word in sequences. Dim: (w_length, hidden_size)\n",
    "            \n",
    "    return torch.cat(h_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = get_latent(model, sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFA Extraction Baseline\n",
    "This part is the one you need to change to put your own algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general idea of this algorithm is to parse words into the RNN and store the hidden states then run the k-means algorithm\n",
    "on these vectors to get the state of the DFA. To find the transition, we use the centroid of\n",
    "each cluster as starting point for the RNN and look in which cluster we arrive for each letter. The information\n",
    "about the final states is obtained by feeding the centroid vector to the output layer(s), and initial state is an\n",
    "initial distribution over the abstract states.\n",
    "\n",
    "<ol>\n",
    "<li> <b>Collecting hidden state vectors</b> to get space <i>H</i> (Lines 1-6). At the first part,  we retrieve all hidden state values from RNN on the set of input sequences <i>X</i> and store them as <i>H</i> (Line 6). Moreover, we take out the hidden state values from all layers and directions, in the case of LSTM also retrieve cell state values.</li>\n",
    "<li><b>Fitting k-means model</b> on the latent values <i>H</i> to get the clusters <i>Q</i>  of k-means model (Lines 8-9)</li>\n",
    "<li><p> <b>Creating DFA set:</b> transition matrices E_&sigma;, final <i>F</i> and initial <i>I</i> states  (Lines 11-25):</p>\n",
    "<ul>\n",
    "<li>  <b> Transition matrices E_&sigma; (Lines 11-26)</b>. Firstly, we initialize zero matrices and vectors (Lines 11-15) and after to fill transition matrices we use two loops (Lines 16-17) that help to go through all amount of clusters and all elements of vocabulary. The next step is getting\n",
    "    the output features <i>out</i> from the last layer of the RNN  and <i>h</i> that contains the final hidden state (and cell state in the case of LSTM)\n",
    "    from all layers and directions (Line 18).\n",
    "     After getting an id of centroid <i>q'</i>  by classifying every <i>h</i> value with k-means model, we use <i>q'</i> to fill E_&sigma; with 1's (Lines 19-20).</li>\n",
    "<li><b> Final state vector <i>F</i> (Lines 21-23)</b>. To get the final state we use <i>out</i> on dense layer of trained RNN (Line 21) and if   <i>sigmoid(dense_out)</i> greater than or equal 0.5 fill <i>F[q]</i> with 1, otherwise it remains zero (Lines 22-23). \n",
    "<li> <b>Initial state vector <i>I</i> (Lines 24-26)</b>. To get id of element what be filled with 1, we classify <i>h_0</i> by k-means model and fill <i>I</i>. After when everything is done, we are able to create DFA set that contains <i>(I, F, E_&sigma;)</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"algo_DFA.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Algorithm\n",
    "And now we code our baseline Algorithm\n",
    "\n",
    "We use the data structure for Automaton that can be found in the toolbox *scikit-splearn* (more info at https://remieyraud.github.io/scikit-splearn/).\n",
    "\n",
    "The toolbox can easily be installed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-splearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from splearn.automaton import Automaton\n",
    "\n",
    "def extract_DFA(model, latent_values, n_clusters, nb_letters, random_state = 0):\n",
    "\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        RNN model\n",
    "    latent_values : stack of hidden states observed\n",
    "    n_clusters : int\n",
    "        Amount of clusters for K-means algorithm\n",
    "    nb_letters : list\n",
    "        List of the symbols in the sequences of the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DFA : object\n",
    "        Returns a DFA        \n",
    "    \"\"\"\n",
    "    \n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=n_clusters, n_init=5, random_state = random_state)\n",
    "    kmeans.fit(latent_values.cpu())\n",
    "\n",
    "\n",
    "    Q = kmeans.cluster_centers_ # line 9 / centroids representing \n",
    "    I = torch.zeros(n_clusters) # line 11\n",
    "    F = torch.zeros(n_clusters) # line 11\n",
    "    trans_list = []  #line 12 - initialisation of the list of transition matrices\n",
    "\n",
    "    for i in range(nb_letters):   # line 13\n",
    "        trans_matrix = np.zeros((n_clusters, n_clusters)) # line 14\n",
    "        trans_list.append(trans_matrix) # line 15\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        D = 2 if model.bidirectional else 1\n",
    "        LSTM = model.hides_pairs #True if each layer has hidden state formated as (h,c), that is, is made of LSTM cells\n",
    "        \n",
    "        for q in range(n_clusters): #line 16 \n",
    "            #Get the hidden state q corresponding to the centroid under consideration\n",
    "            centroid = torch.Tensor(Q[q])\n",
    "            \n",
    "            \"\"\"Format the centroid into the model latent dimension to use it as a starting state in the model\"\"\"\n",
    "            if LSTM: #\n",
    "                #Get values for each layers\n",
    "                state = centroid.view(model.n_layers,-1)\n",
    "                #get the hidden state of each layer separated from the carry\n",
    "                (h, c) = torch.tensor_split(state, 2, dim=-1)\n",
    "                h = h.unbind(0)\n",
    "                #Take care of the potential bidirectional behavior\n",
    "                h = [x.view(D, -1) for x in h]\n",
    "                \n",
    "                #same for the carry\n",
    "                c = c.unbind(0)\n",
    "                c = [x.view(D, -1) for x in c]\n",
    "                \n",
    "                h_centroid = list(zip(h,c))\n",
    "            else:\n",
    "                #Get values for each layers\n",
    "                state = centroid.view(model.n_layers,-1)\n",
    "                h = state.unbind(0)\n",
    "                #Take care of the potential bidirectional behavior\n",
    "                h_centroid = [x.view(D, -1) for x in h]\n",
    "                \n",
    "            for letter in range(nb_letters): # line 17               \n",
    "                \"\"\"Get the hidden state reached from the centroid using the current letter\"\"\"\n",
    "                \n",
    "                #First we encode the letter\n",
    "                one_hot_l = model.one_hot_encode([letter])\n",
    "                \n",
    "                #Then we forward this letter inside the RNN from the hidden state corresponding to the centroid    \n",
    "                if LSTM: \n",
    "                    output, h_c_list = model(one_hot_l, h_centroid)\n",
    "                    #h_c_list is the state reached. We format it in a vector following the same way that get_latent()\n",
    "                    v = [torch.cat(tuple(map(torch.flatten, x)), dim=-1) for x in h_c_list]\n",
    "                    h_reach = torch.cat(v)\n",
    "                else:\n",
    "                    output, h_list = model(one_hot_l, h_centroid)\n",
    "                    #h_list is the state reached from centroid using current letter. We format it in the same way that get_latent()\n",
    "                    v = [torch.flatten(x) for x in h_list]\n",
    "                    h_reach = torch.cat(v)\n",
    "                                                \n",
    "                #transform hidden state obtained to k_means format\n",
    "                h_kmeans = h_reach\n",
    "                                                            \n",
    "                #get reached cluster\n",
    "                q_prime = kmeans.predict(h_kmeans.reshape(1, -1)) #lines 19 - 20\n",
    "                q_prime = q_prime[0]\n",
    "                                                \n",
    "                #Modify correspond Automata transition matrix\n",
    "                trans_list[letter][q][q_prime] = 1 #line 21\n",
    "            \n",
    "            \"\"\"Is the state accepting? That is, what is the output of the dense layr(s) from the centroid\"\"\" \n",
    "            #Format of dense is different from h_centroid because only the last hidden layer is fed to dense one(s) - without c for LSTM\n",
    "            h = state[-1,:model.neurons_per_layer*D]\n",
    "            out = model.feed_dense(h)\n",
    "                                                 \n",
    "            F[q] = out.argmax()\n",
    "            \n",
    "        \"\"\"Who is the initial state?\"\"\"\n",
    "        #Initial state is always 0's, but the dimensions vary with the architecture. Easiest way to have it in the needed k-means format: \n",
    "        h_0 = torch.zeros_like(centroid)                                    \n",
    "        q_0 = kmeans.predict(h_0.reshape(1, -1))[0]\n",
    "        I[q_0]=1\n",
    "\n",
    "    DFA = Automaton(nbL=nb_letters, nbS=n_clusters, initial = np.array(I, dtype = float), final=np.array(F, dtype = float),   # line 28\n",
    "                transitions=trans_list, type='classic')\n",
    "\n",
    "    return DFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the DFA extraction baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction the DFA\n",
    "DFA = extract_DFA(model, H, n_clusters = 12, nb_letters = nb_letters)\n",
    "print(\"Number of states of the extracted DFA:\", DFA.initial.shape[0])\n",
    "print(\"Output on example:\", DFA.val(sequences[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Save extracted DFA as a MLFlow Model. This is the creation of the model needed for the submission to the competition. \n",
    "\n",
    "The only thing to do is to define a function that takes a sequence as a list of integers and returns the value given to this sequence to the sequence. Your model is **NOT** a parameter of this function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq):\n",
    "    return 0 if DFA.val(seq) < 0.5 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and submit \n",
    "This is the creation of the model needed for the submission to the competition: you just have to run this cell. It will create in your current directory an **archive**  that you can then submit on the competition website.\n",
    "\n",
    "**You should NOT modify this part, just run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit_tools import save_function\n",
    "\n",
    "save_function(predict, alphabet_size=nb_letters, prefix=f'dataset{TRACK}.{DATASET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For fun, show DFA graphical representation\n",
    "You may need to install the graphviz library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot =DFA.get_dot(threshold = 0.01, title = 'dotfile')\n",
    "# To display the dot string, one can use graphviz:\n",
    "from graphviz import Source\n",
    "src = Source(dot)\n",
    "src.render('dotfile' + '.gv', view=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NXdKde0kt3FR",
    "eKzzh3hot9vZ",
    "BMQF46fnw1Zk"
   ],
   "name": "PFA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "122e9f251b80a4a76e7262659287020d96f7188da42b39e3d812967db6c8742d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
